# run_quantization.py
"""è¿è¡ŒMASt3R-SLAMæ¨¡å‹é‡åŒ–çš„ä¸»è„šæœ¬"""

import argparse
import sys
import os
from pathlib import Path
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–é¡¹"""
    missing_deps = []
    
    try:
        import tensorrt as trt
        print(f"âœ“ TensorRTç‰ˆæœ¬: {trt.__version__}")
    except ImportError:
        missing_deps.append("tensorrt")
    
    try:
        import onnx
        print(f"âœ“ ONNXç‰ˆæœ¬: {onnx.__version__}")
    except ImportError:
        missing_deps.append("onnx")
    
    if not torch.cuda.is_available():
        print("âœ— CUDAä¸å¯ç”¨")
        return False
    else:
        print(f"âœ“ CUDAå¯ç”¨, è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
    
    if missing_deps:
        print(f"âœ— ç¼ºå°‘ä¾èµ–: {', '.join(missing_deps)}")
        print("è¯·å®‰è£…: pip install tensorrt onnx")
        return False
    
    return True

def main():
    parser = argparse.ArgumentParser(description="MASt3R-SLAMæ¨¡å‹é‡åŒ–å·¥å…·")
    parser.add_argument("--precision", choices=["fp16", "int8"], default="int8",
                       help="é‡åŒ–ç²¾åº¦ (é»˜è®¤: int8)")
    parser.add_argument("--dataset", default="datasets/tum/rgbd_dataset_freiburg1_desk",
                       help="æ ¡å‡†æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--calibration-samples", type=int, default=500,
                       help="æ ¡å‡†æ ·æœ¬æ•°é‡")
    parser.add_argument("--output-dir", default="tensorrt_engines",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--benchmark", action="store_true",
                       help="è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument("--config", default=None,
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return
    
    # å¯¼å…¥é‡åŒ–æ¨¡å—
    try:
        from quantization_config import QuantizationConfig, MASt3RQuantizationManager
    except ImportError as e:
        print(f"å¯¼å…¥é‡åŒ–æ¨¡å—å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿quantization_config.pyåœ¨æ­£ç¡®ä½ç½®")
        return
    
    # åŠ è½½æˆ–åˆ›å»ºé…ç½®
    if args.config and Path(args.config).exists():
        config = QuantizationConfig.load_config(args.config)
        print(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    else:
        config = QuantizationConfig(
            precision=args.precision,
            calibration_dataset=args.dataset,
            calibration_samples=args.calibration_samples,
            output_dir=args.output_dir
        )
        print("ä½¿ç”¨é»˜è®¤é…ç½®")
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print(f"\n=== é‡åŒ–é…ç½® ===")
    print(f"ç²¾åº¦: {config.precision}")
    print(f"æ ¡å‡†æ•°æ®é›†: {config.calibration_dataset}")
    print(f"æ ¡å‡†æ ·æœ¬æ•°: {config.calibration_samples}")
    print(f"è¾“å‡ºç›®å½•: {config.output_dir}")
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    if not Path(config.calibration_dataset).exists():
        print(f"âœ— æ•°æ®é›†ä¸å­˜åœ¨: {config.calibration_dataset}")
        print("è¯·ä¸‹è½½æ•°æ®é›†æˆ–æŒ‡å®šæ­£ç¡®è·¯å¾„")
        return
    
    # æ‰§è¡Œé‡åŒ–
    try:
        manager = MASt3RQuantizationManager(config)
        quantized_model, engine_path = manager.quantize_model()
        
        if quantized_model and engine_path:
            print(f"\nğŸ‰ é‡åŒ–æˆåŠŸå®Œæˆ!")
            print(f"å¼•æ“æ–‡ä»¶: {engine_path}")
            
            # ä¿å­˜é…ç½®æ–‡ä»¶
            config_path = Path(config.output_dir) / "quantization_config.yaml"
            config.save_config(str(config_path))
            print(f"é…ç½®å·²ä¿å­˜: {config_path}")
            
            # è¿è¡ŒåŸºå‡†æµ‹è¯•
            if args.benchmark:
                print("\nè¿è¡Œé¢å¤–åŸºå‡†æµ‹è¯•...")
                run_detailed_benchmark(quantized_model, config)
            
            # ç”Ÿæˆä½¿ç”¨è¯´æ˜
            generate_usage_instructions(engine_path)
        else:
            print("âœ— é‡åŒ–å¤±è´¥")
            
    except Exception as e:
        print(f"é‡åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def run_detailed_benchmark(quantized_model, config):
    """è¿è¡Œè¯¦ç»†çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    import time
    import numpy as np
    
    print("=== è¯¦ç»†æ€§èƒ½æµ‹è¯• ===")
    
    # ä¸åŒæ‰¹æ¬¡å¤§å°æµ‹è¯•
    batch_sizes = [1, 2, 4] if torch.cuda.get_device_properties(0).total_memory > 8e9 else [1, 2]
    
    for batch_size in batch_sizes:
        print(f"\næµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        test_img = torch.randn(batch_size, 3, config.img_size, config.img_size).cuda()
        test_shape = torch.tensor([[config.img_size, config.img_size]] * batch_size)
        
        # é¢„çƒ­
        for _ in range(10):
            with torch.no_grad():
                _ = quantized_model._encode_image(test_img, test_shape)
        
        # æµ‹è¯•
        times = []
        torch.cuda.synchronize()
        
        for _ in range(50):
            start = time.time()
            with torch.no_grad():
                _ = quantized_model._encode_image(test_img, test_shape)
            torch.cuda.synchronize()
            times.append(time.time() - start)
        
        times = np.array(times)
        print(f"  å¹³å‡æ—¶é—´: {np.mean(times):.4f}s")
        print(f"  æ ‡å‡†å·®: {np.std(times):.4f}s")
        print(f"  ååé‡: {batch_size/np.mean(times):.2f} imgs/s")

def generate_usage_instructions(engine_path):
    """ç”Ÿæˆä½¿ç”¨è¯´æ˜"""
    print(f"\n=== ä½¿ç”¨è¯´æ˜ ===")
    print("1. æ›´æ–°mast3r_slam/mast3r_utils.pyï¼Œæ·»åŠ ä»¥ä¸‹ä»£ç :")
    
    usage_code = f'''
# åœ¨load_mast3rå‡½æ•°åæ·»åŠ :
def load_mast3r_quantized(engine_path="{engine_path}", device="cuda"):
    """åŠ è½½é‡åŒ–ç‰ˆæœ¬çš„MASt3Ræ¨¡å‹"""
    from integrate_tensorrt import QuantizedMASt3RModel, QuantizationConfig
    
    # åŠ è½½åŸå§‹æ¨¡å‹
    original_model = load_mast3r(device=device)
    
    # åˆ›å»ºé‡åŒ–é…ç½®
    config = QuantizationConfig(device=device)
    
    # åˆ›å»ºé‡åŒ–æ¨¡å‹
    quantized_model = QuantizedMASt3RModel(original_model, engine_path, config)
    
    return quantized_model

# ä¿®æ”¹ç°æœ‰çš„load_mast3rå‡½æ•°:
def load_mast3r(path=None, device="cuda", use_quantized=False, engine_path=None):
    if use_quantized and engine_path and Path(engine_path).exists():
        return load_mast3r_quantized(engine_path, device)
    
    # åŸæœ‰ä»£ç ...
    weights_path = (
        "checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth"
        if path is None else path
    )
    model = AsymmetricMASt3R.from_pretrained(weights_path).to(device)
    return model
'''
    
    print(usage_code)
    
    print("\n2. åœ¨main.pyä¸­ä½¿ç”¨é‡åŒ–æ¨¡å‹:")
    print(f'''
# åœ¨main.pyä¸­ä¿®æ”¹æ¨¡å‹åŠ è½½éƒ¨åˆ†:
model = load_mast3r(device=device, use_quantized=True, engine_path="{engine_path}")
''')
    
    print("\n3. æˆ–è€…ç›´æ¥æ›¿æ¢ç°æœ‰çš„load_mast3rè°ƒç”¨:")
    print(f'''
from mast3r_slam.mast3r_utils import load_mast3r_quantized
model = load_mast3r_quantized("{engine_path}", device=device)
''')

# modify_main_for_quantization.py
"""ä¿®æ”¹main.pyä»¥æ”¯æŒé‡åŒ–æ¨¡å‹çš„è„šæœ¬"""

def modify_main_py(engine_path: str):
    """ä¿®æ”¹main.pyæ–‡ä»¶ä»¥æ”¯æŒé‡åŒ–"""
    
    main_py_path = Path("main.py")
    if not main_py_path.exists():
        print("main.pyæ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    # è¯»å–åŸæ–‡ä»¶
    with open(main_py_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # å¤‡ä»½åŸæ–‡ä»¶
    backup_path = main_py_path.with_suffix('.py.backup')
    with open(backup_path, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"å·²å¤‡ä»½åŸæ–‡ä»¶åˆ°: {backup_path}")
    
    # ä¿®æ”¹å†…å®¹
    # 1. æ·»åŠ é‡åŒ–æ¨¡å‹æ”¯æŒ
    import_addition = '''from pathlib import Path
import argparse
import datetime
import pathlib
import sys
import time
import cv2
import lietorch
import torch
import tqdm
import yaml
from mast3r_slam.global_opt import FactorGraph

from mast3r_slam.config import load_config, config, set_global_config
from mast3r_slam.dataloader import Intrinsics, load_dataset
import mast3r_slam.evaluate as eval
from mast3r_slam.frame import Mode, SharedKeyframes, SharedStates, create_frame
from mast3r_slam.mast3r_utils import (
    load_mast3r,
    load_retriever,
    mast3r_inference_mono,
)
from mast3r_slam.multiprocess_utils import new_queue, try_get_msg
from mast3r_slam.tracker import FrameTracker
from mast3r_slam.visualization import WindowMsg, run_visualization
import torch.multiprocessing as mp

# é‡åŒ–æ¨¡å‹æ”¯æŒ
try:
    from integrate_tensorrt import QuantizedMASt3RModel, QuantizationConfig
    QUANTIZATION_AVAILABLE = True
except ImportError:
    QUANTIZATION_AVAILABLE = False
    print("é‡åŒ–æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹")
'''
    
    # 2. ä¿®æ”¹å‚æ•°è§£æå™¨
    parser_addition = '''
    parser.add_argument("--use-quantized", action="store_true", help="ä½¿ç”¨é‡åŒ–æ¨¡å‹")
    parser.add_argument("--engine-path", default="tensorrt_engines/mast3r_vit_encoder_int8.trt", 
                       help="TensorRTå¼•æ“è·¯å¾„")'''
    
    # 3. ä¿®æ”¹æ¨¡å‹åŠ è½½éƒ¨åˆ†
    model_loading_modification = f'''
    # åŠ è½½æ¨¡å‹ (æ”¯æŒé‡åŒ–)
    if args.use_quantized and QUANTIZATION_AVAILABLE:
        engine_path = Path(args.engine_path)
        if engine_path.exists():
            print(f"åŠ è½½é‡åŒ–æ¨¡å‹: {{engine_path}}")
            original_model = load_mast3r(device=device)
            quant_config = QuantizationConfig(device=device)
            model = QuantizedMASt3RModel(original_model, str(engine_path), quant_config)
        else:
            print(f"é‡åŒ–å¼•æ“ä¸å­˜åœ¨: {{engine_path}}, ä½¿ç”¨åŸå§‹æ¨¡å‹")
            model = load_mast3r(device=device)
    else:
        model = load_mast3r(device=device)
    '''
    
    # åº”ç”¨ä¿®æ”¹
    # æ›¿æ¢å¯¼å…¥éƒ¨åˆ†
    content = content.replace(
        "import torch.multiprocessing as mp",
        import_addition
    )
    
    # æ·»åŠ å‚æ•°
    content = content.replace(
        'parser.add_argument("--calib", default="")',
        'parser.add_argument("--calib", default="")\n' + parser_addition
    )
    
    # æ›¿æ¢æ¨¡å‹åŠ è½½
    content = content.replace(
        "model = load_mast3r(device=device)",
        model_loading_modification
    )
    
    # å†™å…¥ä¿®æ”¹åçš„æ–‡ä»¶
    with open(main_py_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"å·²ä¿®æ”¹main.pyæ–‡ä»¶ï¼Œå¤‡ä»½ä¿å­˜åœ¨: {backup_path}")
    print("ç°åœ¨å¯ä»¥ä½¿ç”¨ --use-quantized å‚æ•°è¿è¡Œé‡åŒ–ç‰ˆæœ¬")

if __name__ == "__main__":
    main()
