# mast3r_optimization.py
"""é’ˆå¯¹MASt3Ræ¨¡å‹ç»“æ„çš„ä¸“ç”¨ä¼˜åŒ–è„šæœ¬"""

import torch
import torch.nn as nn
import numpy as np
import os
import sys
from pathlib import Path
import time
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

def analyze_model_structure(model):
    """åˆ†æMASt3Ræ¨¡å‹ç»“æ„"""
    print("=== åˆ†æMASt3Ræ¨¡å‹ç»“æ„ ===")
    
    print("ä¸»è¦ç»„ä»¶:")
    for name, module in model.named_children():
        print(f"  {name}: {type(module).__name__}")
        
        # å¦‚æœæ˜¯ç¼–ç å™¨ç›¸å…³ï¼Œè¿›ä¸€æ­¥åˆ†æ
        if 'enc' in name.lower() or 'backbone' in name.lower():
            print(f"    è¯¦ç»†ç»“æ„: {module}")
    
    # æ£€æŸ¥ç¼–ç ç›¸å…³æ–¹æ³•
    print("\nç¼–ç ç›¸å…³æ–¹æ³•:")
    methods = [method for method in dir(model) if 'enc' in method.lower() or method.startswith('_encode')]
    for method in methods:
        print(f"  {method}")
    
    return

def find_encoder_components(model):
    """æŸ¥æ‰¾MASt3Rçš„ç¼–ç å™¨ç»„ä»¶"""
    print("æŸ¥æ‰¾ç¼–ç å™¨ç»„ä»¶...")
    
    # æ£€æŸ¥å¯èƒ½çš„ç¼–ç å™¨å±æ€§
    encoder_candidates = []
    
    for name in dir(model):
        if not name.startswith('_'):
            attr = getattr(model, name)
            if isinstance(attr, nn.Module):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«attentionæˆ–transformerç»“æ„
                has_attention = any('attention' in str(type(m)).lower() or 'transformer' in str(type(m)).lower() 
                                  for m in attr.modules())
                if has_attention:
                    encoder_candidates.append((name, attr))
                    print(f"æ‰¾åˆ°å€™é€‰ç¼–ç å™¨: {name} - {type(attr).__name__}")
    
    # ç‰¹æ®Šæ£€æŸ¥MASt3Rå¯èƒ½çš„ç»“æ„
    special_attrs = ['backbone', 'enc_backbone', 'encoder_backbone', 'vision_transformer']
    for attr_name in special_attrs:
        if hasattr(model, attr_name):
            attr = getattr(model, attr_name)
            print(f"æ‰¾åˆ°ç‰¹æ®Šå±æ€§: {attr_name} - {type(attr).__name__}")
            encoder_candidates.append((attr_name, attr))
    
    return encoder_candidates

def create_encoding_wrapper(model):
    """åˆ›å»ºç¼–ç åŒ…è£…å™¨ï¼Œç›´æ¥ä½¿ç”¨_encode_imageæ–¹æ³•"""
    print("åˆ›å»ºç¼–ç åŒ…è£…å™¨...")
    
    class MASt3REncodingWrapper(nn.Module):
        def __init__(self, original_model):
            super().__init__()
            self.original_model = original_model
            
        def forward(self, x):
            # ç›´æ¥ä½¿ç”¨MASt3Rçš„ç¼–ç æ–¹æ³•
            B, C, H, W = x.shape
            true_shape = torch.tensor([[H, W]], device=x.device)
            
            feat, pos, _ = self.original_model._encode_image(x, true_shape)
            return feat
    
    wrapper = MASt3REncodingWrapper(model)
    print("âœ“ ç¼–ç åŒ…è£…å™¨åˆ›å»ºæˆåŠŸ")
    return wrapper

def test_encoding_wrapper(wrapper):
    """æµ‹è¯•ç¼–ç åŒ…è£…å™¨"""
    print("æµ‹è¯•ç¼–ç åŒ…è£…å™¨...")
    
    test_input = torch.randn(1, 3, 512, 512).cuda()
    
    try:
        with torch.no_grad():
            output = wrapper(test_input)
            print(f"âœ“ åŒ…è£…å™¨è¾“å‡ºå½¢çŠ¶: {output.shape}")
            return True
    except Exception as e:
        print(f"åŒ…è£…å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def optimize_with_torchscript(wrapper):
    """ä½¿ç”¨TorchScriptä¼˜åŒ–"""
    print("ä½¿ç”¨TorchScriptä¼˜åŒ–...")
    
    try:
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        example_input = torch.randn(1, 3, 512, 512).cuda()
        
        # ä½¿ç”¨torch.jit.trace
        print("æ‰§è¡Œtorch.jit.trace...")
        traced_model = torch.jit.trace(wrapper, example_input)
        
        # ä¼˜åŒ–
        print("åº”ç”¨TorchScriptä¼˜åŒ–...")
        optimized_model = torch.jit.optimize_for_inference(traced_model)
        
        print("âœ“ TorchScriptä¼˜åŒ–æˆåŠŸ")
        return optimized_model
        
    except Exception as e:
        print(f"TorchScriptä¼˜åŒ–å¤±è´¥: {e}")
        
        # å°è¯•scripting
        try:
            print("å°è¯•torch.jit.script...")
            scripted_model = torch.jit.script(wrapper)
            optimized_model = torch.jit.optimize_for_inference(scripted_model)
            print("âœ“ TorchScript scriptingæˆåŠŸ")
            return optimized_model
        except Exception as e2:
            print(f"TorchScript scriptingä¹Ÿå¤±è´¥: {e2}")
            return None

def try_compilation_optimization(wrapper):
    """å°è¯•PyTorch 2.0ç¼–è¯‘ä¼˜åŒ–"""
    print("å°è¯•PyTorchç¼–è¯‘ä¼˜åŒ–...")
    
    try:
        # æ£€æŸ¥PyTorchç‰ˆæœ¬
        torch_version = torch.__version__
        print(f"PyTorchç‰ˆæœ¬: {torch_version}")
        
        if hasattr(torch, 'compile'):
            print("ä½¿ç”¨torch.compileä¼˜åŒ–...")
            compiled_model = torch.compile(wrapper, mode="reduce-overhead")
            print("âœ“ PyTorchç¼–è¯‘ä¼˜åŒ–æˆåŠŸ")
            return compiled_model
        else:
            print("PyTorchç‰ˆæœ¬ä¸æ”¯æŒtorch.compile")
            return None
            
    except Exception as e:
        print(f"ç¼–è¯‘ä¼˜åŒ–å¤±è´¥: {e}")
        return None

def benchmark_optimizations(original_model, optimized_models):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\n=== æ€§èƒ½åŸºå‡†æµ‹è¯• ===")
    
    test_input = torch.randn(1, 3, 512, 512).cuda()
    test_shape = torch.tensor([[512, 512]])
    
    # é¢„çƒ­
    print("é¢„çƒ­...")
    for _ in range(10):
        with torch.no_grad():
            _ = original_model._encode_image(test_input, test_shape)
    
    # æµ‹è¯•åŸå§‹æ¨¡å‹
    print("æµ‹è¯•åŸå§‹æ¨¡å‹...")
    torch.cuda.synchronize()
    start_time = time.time()
    
    for _ in range(100):
        with torch.no_grad():
            _ = original_model._encode_image(test_input, test_shape)
    
    torch.cuda.synchronize()
    original_time = time.time() - start_time
    
    print(f"åŸå§‹æ¨¡å‹æ—¶é—´: {original_time:.4f}s (å¹³å‡: {original_time/100:.4f}s)")
    
    # æµ‹è¯•ä¼˜åŒ–æ¨¡å‹
    results = {"original": original_time}
    
    for name, opt_model in optimized_models.items():
        if opt_model is None:
            continue
            
        print(f"æµ‹è¯•{name}...")
        
        # é¢„çƒ­ä¼˜åŒ–æ¨¡å‹
        for _ in range(10):
            with torch.no_grad():
                _ = opt_model(test_input)
        
        torch.cuda.synchronize()
        start_time = time.time()
        
        for _ in range(100):
            with torch.no_grad():
                _ = opt_model(test_input)
        
        torch.cuda.synchronize()
        opt_time = time.time() - start_time
        
        results[name] = opt_time
        speedup = original_time / opt_time
        print(f"{name}æ—¶é—´: {opt_time:.4f}s (å¹³å‡: {opt_time/100:.4f}s) - åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    return results

def save_best_model(optimized_models, results):
    """ä¿å­˜æœ€ä½³ä¼˜åŒ–æ¨¡å‹"""
    print("\n=== ä¿å­˜æœ€ä½³æ¨¡å‹ ===")
    
    best_model = None
    best_name = None
    best_speedup = 1.0
    
    original_time = results["original"]
    
    for name, model in optimized_models.items():
        if model is None or name not in results:
            continue
            
        speedup = original_time / results[name]
        if speedup > best_speedup:
            best_speedup = speedup
            best_model = model
            best_name = name
    
    if best_model is not None:
        save_path = f"optimized_mast3r_{best_name}.pt"
        torch.jit.save(best_model, save_path)
        print(f"âœ“ æœ€ä½³ä¼˜åŒ–æ¨¡å‹å·²ä¿å­˜: {save_path}")
        print(f"æœ€ä½³ä¼˜åŒ–æ–¹æ³•: {best_name} (åŠ é€Ÿæ¯”: {best_speedup:.2f}x)")
        return save_path, best_name, best_speedup
    else:
        print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä¼˜åŒ–æ¨¡å‹")
        return None, None, 1.0

def create_integration_code(model_path, method_name, speedup):
    """ç”Ÿæˆé›†æˆä»£ç """
    print(f"\n=== é›†æˆä»£ç  ===")
    
    code = f'''
# åœ¨main.pyä¸­æ·»åŠ ä»¥ä¸‹ä»£ç æ¥ä½¿ç”¨ä¼˜åŒ–æ¨¡å‹

def load_optimized_mast3r():
    """åŠ è½½ä¼˜åŒ–ç‰ˆæœ¬çš„MASt3Ræ¨¡å‹"""
    try:
        # åŠ è½½åŸå§‹æ¨¡å‹
        original_model = load_mast3r(device=device)
        
        # æ£€æŸ¥ä¼˜åŒ–æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if os.path.exists("{model_path}"):
            print("åŠ è½½ä¼˜åŒ–çš„MASt3Ræ¨¡å‹ ({method_name}, {speedup:.2f}xåŠ é€Ÿ)...")
            optimized_encoder = torch.jit.load("{model_path}")
            
            # åˆ›å»ºæ··åˆæ¨¡å‹ç±»
            class OptimizedMASt3R:
                def __init__(self, original_model, optimized_encoder):
                    self.original_model = original_model
                    self.optimized_encoder = optimized_encoder
                    
                    # ä¿ç•™æ‰€æœ‰åŸå§‹å±æ€§
                    self._decoder = original_model._decoder
                    self._downstream_head = original_model._downstream_head
                
                def _encode_image(self, img, true_shape):
                    """ä½¿ç”¨ä¼˜åŒ–çš„ç¼–ç å™¨"""
                    try:
                        # ä½¿ç”¨ä¼˜åŒ–ç¼–ç å™¨
                        feat = self.optimized_encoder(img)
                        
                        # ç”Ÿæˆä½ç½®ç¼–ç ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                        h, w = true_shape[0].item(), true_shape[1].item()
                        pos = torch.zeros(1, feat.shape[1], 2, device=img.device, dtype=torch.long)
                        
                        return feat, pos, None
                    except Exception as e:
                        print(f"ä¼˜åŒ–ç¼–ç å™¨å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•: {{e}}")
                        return self.original_model._encode_image(img, true_shape)
                
                def __getattr__(self, name):
                    """ä»£ç†å…¶ä»–å±æ€§åˆ°åŸå§‹æ¨¡å‹"""
                    return getattr(self.original_model, name)
            
            optimized_model = OptimizedMASt3R(original_model, optimized_encoder)
            print("âœ“ ä¼˜åŒ–æ¨¡å‹é›†æˆæˆåŠŸ")
            return optimized_model
        
        else:
            print("ä¼˜åŒ–æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹")
            return original_model
            
    except Exception as e:
        print(f"ä¼˜åŒ–æ¨¡å‹åŠ è½½å¤±è´¥: {{e}}")
        return load_mast3r(device=device)

# åœ¨main.pyä¸­æ›¿æ¢æ¨¡å‹åŠ è½½è¡Œ:
# model = load_mast3r(device=device)
# æ”¹ä¸º:
model = load_optimized_mast3r()
'''
    
    print(code)
    
    # ä¿å­˜é›†æˆä»£ç åˆ°æ–‡ä»¶
    with open("integration_code.py", "w") as f:
        f.write(code)
    print("é›†æˆä»£ç å·²ä¿å­˜åˆ°: integration_code.py")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--analyze", action="store_true", help="åªåˆ†ææ¨¡å‹ç»“æ„")
    
    args = parser.parse_args()
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        from mast3r.model import AsymmetricMASt3R
        
        model_path = "checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth"
        model = AsymmetricMASt3R.from_pretrained(model_path).cuda().eval()
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 2. åˆ†ææ¨¡å‹ç»“æ„
        if args.analyze:
            analyze_model_structure(model)
            find_encoder_components(model)
            return
        
        # 3. åˆ›å»ºç¼–ç åŒ…è£…å™¨
        wrapper = create_encoding_wrapper(model)
        
        # 4. æµ‹è¯•åŒ…è£…å™¨
        if not test_encoding_wrapper(wrapper):
            print("åŒ…è£…å™¨æµ‹è¯•å¤±è´¥ï¼Œé€€å‡º")
            return
        
        # 5. å°è¯•ä¸åŒä¼˜åŒ–æ–¹æ³•
        optimized_models = {}
        
        # TorchScriptä¼˜åŒ–
        print("\n=== TorchScriptä¼˜åŒ– ===")
        torchscript_model = optimize_with_torchscript(wrapper)
        optimized_models["torchscript"] = torchscript_model
        
        # PyTorchç¼–è¯‘ä¼˜åŒ–
        print("\n=== PyTorchç¼–è¯‘ä¼˜åŒ– ===")
        compiled_model = try_compilation_optimization(wrapper)
        optimized_models["compiled"] = compiled_model
        
        # 6. æ€§èƒ½æµ‹è¯•
        results = benchmark_optimizations(model, optimized_models)
        
        # 7. ä¿å­˜æœ€ä½³æ¨¡å‹
        best_path, best_method, best_speedup = save_best_model(optimized_models, results)
        
        # 8. ç”Ÿæˆé›†æˆä»£ç 
        if best_path:
            create_integration_code(best_path, best_method, best_speedup)
        
        print(f"\nğŸ‰ ä¼˜åŒ–å®Œæˆ!")
        if best_speedup > 1.1:
            print(f"æœ€ä½³åŠ é€Ÿæ¯”: {best_speedup:.2f}x")
        else:
            print("æœªè·å¾—æ˜¾è‘—åŠ é€Ÿï¼Œå»ºè®®æ£€æŸ¥ç¡¬ä»¶é…ç½®")
        
    except Exception as e:
        print(f"ä¼˜åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
