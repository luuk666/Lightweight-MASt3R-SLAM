# practical_optimization.py
"""å®ç”¨çš„MASt3Rä¼˜åŒ–æ–¹æ¡ˆ - é€šè¿‡å†…å­˜å’Œè®¡ç®—ä¼˜åŒ–è·å¾—åŠ é€Ÿ"""

import torch
import torch.nn as nn
import numpy as np
import os
import sys
from pathlib import Path
import time
import argparse
import contextlib

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

class OptimizedMASt3RWrapper:
    """ä¼˜åŒ–çš„MASt3RåŒ…è£…å™¨"""
    
    def __init__(self, original_model, optimization_config):
        self.original_model = original_model
        self.config = optimization_config
        
        # åº”ç”¨ä¼˜åŒ–è®¾ç½®
        self._apply_optimizations()
        
        # ä¿ç•™åŸå§‹æ¥å£
        self._decoder = original_model._decoder
        self._downstream_head = original_model._downstream_head
        
        # æ€§èƒ½ç»Ÿè®¡
        self.inference_times = []
        self.memory_usage = []
        
    def _apply_optimizations(self):
        """åº”ç”¨å„ç§ä¼˜åŒ–æŠ€æœ¯"""
        
        # 1. è®¾ç½®æ¨¡å‹ä¸ºevalæ¨¡å¼å¹¶ç¦ç”¨æ¢¯åº¦
        self.original_model.eval()
        for param in self.original_model.parameters():
            param.requires_grad = False
        
        # 2. ä½¿ç”¨inference mode
        self.inference_mode = True
        
        # 3. å†…å­˜ä¼˜åŒ–
        if self.config.get('use_half_precision', False):
            self.original_model = self.original_model.half()
            print("âœ“ å¯ç”¨åŠç²¾åº¦æ¨ç†")
        
        # 4. ç¼–è¯‘å…³é”®æ¨¡å—ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        if self.config.get('try_compile_modules', False):
            self._try_compile_modules()
    
    def _try_compile_modules(self):
        """å°è¯•ç¼–è¯‘å•ä¸ªæ¨¡å—"""
        try:
            # å°è¯•ç¼–è¯‘patch_embed
            if hasattr(self.original_model, 'patch_embed'):
                try:
                    self.original_model.patch_embed = torch.compile(
                        self.original_model.patch_embed, 
                        mode="reduce-overhead",
                        dynamic=False
                    )
                    print("âœ“ patch_embedç¼–è¯‘æˆåŠŸ")
                except:
                    print("patch_embedç¼–è¯‘å¤±è´¥")
            
            # å°è¯•ç¼–è¯‘éƒ¨åˆ†enc_blocks
            if hasattr(self.original_model, 'enc_blocks'):
                try:
                    for i, block in enumerate(self.original_model.enc_blocks[:4]):  # åªç¼–è¯‘å‰å‡ å±‚
                        self.original_model.enc_blocks[i] = torch.compile(
                            block, 
                            mode="reduce-overhead",
                            dynamic=False
                        )
                    print("âœ“ å‰4ä¸ªç¼–ç å™¨å—ç¼–è¯‘æˆåŠŸ")
                except:
                    print("ç¼–ç å™¨å—ç¼–è¯‘å¤±è´¥")
                    
        except Exception as e:
            print(f"æ¨¡å—ç¼–è¯‘å¤±è´¥: {e}")
    
    def _encode_image(self, img, true_shape):
        """ä¼˜åŒ–çš„å›¾åƒç¼–ç """
        
        # é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
        if self.config.get('use_amp', False):
            return self._encode_with_amp(img, true_shape)
        elif self.config.get('use_inference_mode', True):
            return self._encode_with_inference_mode(img, true_shape)
        else:
            return self.original_model._encode_image(img, true_shape)
    
    def _encode_with_amp(self, img, true_shape):
        """ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦"""
        with torch.cuda.amp.autocast():
            return self.original_model._encode_image(img, true_shape)
    
    def _encode_with_inference_mode(self, img, true_shape):
        """ä½¿ç”¨æ¨ç†æ¨¡å¼"""
        with torch.inference_mode():
            start_time = time.time()
            result = self.original_model._encode_image(img, true_shape)
            
            # è®°å½•æ€§èƒ½
            inference_time = time.time() - start_time
            self.inference_times.append(inference_time)
            
            return result
    
    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if not self.inference_times:
            return {}
        
        times = np.array(self.inference_times)
        return {
            "num_inferences": len(times),
            "avg_time": np.mean(times),
            "std_time": np.std(times),
            "min_time": np.min(times),
            "max_time": np.max(times),
            "total_time": np.sum(times)
        }
    
    def __getattr__(self, name):
        """ä»£ç†å…¶ä»–å±æ€§"""
        return getattr(self.original_model, name)

def create_optimization_configs():
    """åˆ›å»ºä¸åŒçš„ä¼˜åŒ–é…ç½®"""
    
    configs = {
        "baseline": {
            "use_inference_mode": True,
            "use_half_precision": False,
            "use_amp": False,
            "try_compile_modules": False
        },
        
        "memory_optimized": {
            "use_inference_mode": True,
            "use_half_precision": True,
            "use_amp": False,
            "try_compile_modules": False
        },
        
        "amp_optimized": {
            "use_inference_mode": True,
            "use_half_precision": False,
            "use_amp": True,
            "try_compile_modules": False
        },
        
        "aggressive": {
            "use_inference_mode": True,
            "use_half_precision": True,
            "use_amp": True,
            "try_compile_modules": True
        }
    }
    
    return configs

def benchmark_configurations(model, configs, num_iterations=50):
    """æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½"""
    print("\n=== é…ç½®æ€§èƒ½æµ‹è¯• ===")
    
    test_input = torch.randn(1, 3, 512, 512).cuda()
    test_shape = torch.tensor([[512, 512]])
    
    results = {}
    
    for config_name, config in configs.items():
        print(f"\næµ‹è¯•é…ç½®: {config_name}")
        
        try:
            # åˆ›å»ºä¼˜åŒ–åŒ…è£…å™¨
            optimized_model = OptimizedMASt3RWrapper(model, config)
            
            # é¢„çƒ­
            for _ in range(10):
                _ = optimized_model._encode_image(test_input, test_shape)
            
            # æ¸…ç©ºä¹‹å‰çš„ç»Ÿè®¡
            optimized_model.inference_times.clear()
            
            # æ€§èƒ½æµ‹è¯•
            torch.cuda.synchronize()
            start_time = time.time()
            
            for _ in range(num_iterations):
                _ = optimized_model._encode_image(test_input, test_shape)
            
            torch.cuda.synchronize()
            total_time = time.time() - start_time
            
            # è·å–è¯¦ç»†ç»Ÿè®¡
            stats = optimized_model.get_performance_stats()
            results[config_name] = {
                'total_time': total_time,
                'avg_time': total_time / num_iterations,
                'stats': stats
            }
            
            print(f"  æ€»æ—¶é—´: {total_time:.4f}s")
            print(f"  å¹³å‡æ—¶é—´: {total_time/num_iterations:.4f}s")
            
        except Exception as e:
            print(f"  é…ç½® {config_name} å¤±è´¥: {e}")
            results[config_name] = None
    
    return results

def find_best_configuration(results):
    """æ‰¾åˆ°æœ€ä½³é…ç½®"""
    
    valid_results = {k: v for k, v in results.items() if v is not None}
    
    if not valid_results:
        return None, None
    
    # æŒ‰å¹³å‡æ—¶é—´æ’åº
    best_config = min(valid_results.items(), key=lambda x: x[1]['avg_time'])
    return best_config

def apply_system_optimizations():
    """åº”ç”¨ç³»ç»Ÿçº§ä¼˜åŒ–"""
    print("åº”ç”¨ç³»ç»Ÿçº§ä¼˜åŒ–...")
    
    # PyTorchä¼˜åŒ–è®¾ç½®
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    
    # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
    
    # ç¦ç”¨è°ƒè¯•åŠŸèƒ½
    torch.autograd.set_detect_anomaly(False)
    torch.autograd.profiler.profile(enabled=False)
    
    print("âœ“ ç³»ç»Ÿä¼˜åŒ–å·²åº”ç”¨")

def create_production_model(model, best_config_name, best_config):
    """åˆ›å»ºç”Ÿäº§å°±ç»ªçš„ä¼˜åŒ–æ¨¡å‹"""
    print(f"\n=== åˆ›å»ºç”Ÿäº§æ¨¡å‹ (é…ç½®: {best_config_name}) ===")
    
    optimized_model = OptimizedMASt3RWrapper(model, best_config)
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    model_info = {
        'config_name': best_config_name,
        'config': best_config,
        'optimization_applied': True
    }
    
    return optimized_model, model_info

def generate_integration_code(model_info):
    """ç”Ÿæˆé›†æˆä»£ç """
    
    config_str = str(model_info['config']).replace("'", '"')
    
    code = f'''
# optimized_mast3r_loader.py
"""ä¼˜åŒ–çš„MASt3Ræ¨¡å‹åŠ è½½å™¨"""

import torch
import torch.nn as nn
import time
import numpy as np

class OptimizedMASt3RWrapper:
    """ä¼˜åŒ–çš„MASt3RåŒ…è£…å™¨"""
    
    def __init__(self, original_model, optimization_config):
        self.original_model = original_model
        self.config = optimization_config
        
        # åº”ç”¨ä¼˜åŒ–è®¾ç½®
        self._apply_optimizations()
        
        # ä¿ç•™åŸå§‹æ¥å£
        self._decoder = original_model._decoder
        self._downstream_head = original_model._downstream_head
        
    def _apply_optimizations(self):
        """åº”ç”¨ä¼˜åŒ–æŠ€æœ¯"""
        self.original_model.eval()
        for param in self.original_model.parameters():
            param.requires_grad = False
        
        if self.config.get('use_half_precision', False):
            self.original_model = self.original_model.half()
    
    def _encode_image(self, img, true_shape):
        """ä¼˜åŒ–çš„å›¾åƒç¼–ç """
        if self.config.get('use_amp', False):
            with torch.cuda.amp.autocast():
                return self.original_model._encode_image(img, true_shape)
        elif self.config.get('use_inference_mode', True):
            with torch.inference_mode():
                return self.original_model._encode_image(img, true_shape)
        else:
            return self.original_model._encode_image(img, true_shape)
    
    def __getattr__(self, name):
        return getattr(self.original_model, name)

def load_optimized_mast3r(device="cuda"):
    """åŠ è½½ä¼˜åŒ–çš„MASt3Ræ¨¡å‹"""
    
    # åº”ç”¨ç³»ç»Ÿä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    
    # åŠ è½½åŸå§‹æ¨¡å‹
    from mast3r_slam.mast3r_utils import load_mast3r
    original_model = load_mast3r(device=device)
    
    # æœ€ä½³é…ç½®
    best_config = {config_str}
    
    # åˆ›å»ºä¼˜åŒ–åŒ…è£…å™¨
    optimized_model = OptimizedMASt3RWrapper(original_model, best_config)
    
    print("âœ“ ä¼˜åŒ–MASt3Ræ¨¡å‹åŠ è½½å®Œæˆ (é…ç½®: {model_info['config_name']})")
    return optimized_model

# åœ¨main.pyä¸­ä½¿ç”¨:
# from optimized_mast3r_loader import load_optimized_mast3r
# model = load_optimized_mast3r(device=device)
'''
    
    print("=== é›†æˆä»£ç  ===")
    print(code)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open("optimized_mast3r_loader.py", "w") as f:
        f.write(code)
    
    print("\nâœ“ é›†æˆä»£ç å·²ä¿å­˜åˆ°: optimized_mast3r_loader.py")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--iterations", type=int, default=50, help="æµ‹è¯•è¿­ä»£æ¬¡æ•°")
    
    args = parser.parse_args()
    
    try:
        print("=== MASt3Rå®ç”¨ä¼˜åŒ–æ–¹æ¡ˆ ===")
        
        # 1. åº”ç”¨ç³»ç»Ÿä¼˜åŒ–
        apply_system_optimizations()
        
        # 2. åŠ è½½æ¨¡å‹
        print("\nåŠ è½½MASt3Ræ¨¡å‹...")
        from mast3r.model import AsymmetricMASt3R
        
        model_path = "checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth"
        model = AsymmetricMASt3R.from_pretrained(model_path).cuda().eval()
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 3. æµ‹è¯•åŸå§‹æ€§èƒ½
        print("\næµ‹è¯•åŸå§‹æ¨¡å‹æ€§èƒ½...")
        test_input = torch.randn(1, 3, 512, 512).cuda()
        test_shape = torch.tensor([[512, 512]])
        
        # é¢„çƒ­
        for _ in range(10):
            with torch.no_grad():
                _ = model._encode_image(test_input, test_shape)
        
        # æµ‹è¯•
        torch.cuda.synchronize()
        start_time = time.time()
        for _ in range(args.iterations):
            with torch.no_grad():
                _ = model._encode_image(test_input, test_shape)
        torch.cuda.synchronize()
        baseline_time = time.time() - start_time
        
        print(f"åŸå§‹æ¨¡å‹æ—¶é—´: {baseline_time:.4f}s (å¹³å‡: {baseline_time/args.iterations:.4f}s)")
        
        # 4. æµ‹è¯•ä¸åŒä¼˜åŒ–é…ç½®
        configs = create_optimization_configs()
        results = benchmark_configurations(model, configs, args.iterations)
        
        # 5. æ‰¾åˆ°æœ€ä½³é…ç½®
        best_result = find_best_configuration(results)
        
        if best_result:
            best_name, best_data = best_result
            speedup = baseline_time / best_data['total_time']
            
            print(f"\nğŸ‰ æœ€ä½³é…ç½®: {best_name}")
            print(f"åŠ é€Ÿæ¯”: {speedup:.2f}x")
            print(f"æ—¶é—´å‡å°‘: {(1-1/speedup)*100:.1f}%")
            
            # 6. åˆ›å»ºç”Ÿäº§æ¨¡å‹
            config = configs[best_name]
            optimized_model, model_info = create_production_model(model, best_name, config)
            
            # 7. ç”Ÿæˆé›†æˆä»£ç 
            generate_integration_code(model_info)
            
            print(f"\nâœ… ä¼˜åŒ–å®Œæˆ!")
            print(f"æœ€ä½³æ€§èƒ½æå‡: {speedup:.2f}x")
            print("è¯·ä½¿ç”¨ç”Ÿæˆçš„ optimized_mast3r_loader.py æ¥åŠ è½½ä¼˜åŒ–æ¨¡å‹")
            
        else:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ä¼˜åŒ–é…ç½®")
        
    except Exception as e:
        print(f"ä¼˜åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
